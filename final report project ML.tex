\documentclass[12pt, a4paper]{article}

% --- PACKAGES ---
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}              % Set language to English
\usepackage[a4paper, margin=1in]{geometry} % 1-inch margins
\usepackage{graphicx}                   % Include images
\usepackage{amsmath, amssymb}           % Math symbols
\usepackage{booktabs}                   % Professional tables
\usepackage[hyphens]{url}               % Handle URLs
\usepackage{hyperref}                   % Clickable links
\hypersetup{
    colorlinks=true,
    linkcolor=blue!70!black,
    citecolor=green!70!black,
    urlcolor=blue!70!black
}
\usepackage{xcolor}                     % Colors
\usepackage{float}                      % Force float placement [H]
\usepackage{setspace}                   % Line spacing
\usepackage{titlesec}                   % Customize section titles
\usepackage{parskip}                    % Paragraphs separated by space, not indent
\usepackage{caption}                    % Better captions

% --- CUSTOMIZE SECTION TITLES ---
\titleformat{\section}
  {\normalfont\Large\bfseries\color{blue!30!black}}
  {\thesection}{1em}{}
\titleformat{\subsection}
  {\normalfont\large\bfseries\color{gray!80!black}}
  {\thesubsection}{1em}{}
\titleformat{\subsubsection}
  {\normalfont\normalsize\bfseries}
  {\thesubsubsection}{1em}{}

% --- DOCUMENT INFORMATION ---
\title{
    \vspace{2cm}
    \textbf{IF5 Machine Learning Project: \\ Financial Sentiment Analysis}
    \vspace{1.5cm}
}
\author{
    \textbf{Group Members:} \\
    PERALDI Yasmine \\
    SALIBA Rhea \\
    SERRIER Pierre-Antoine
}
\date{\today}

% --- START DOCUMENT ---
\begin{document}

% --- TITLE PAGE ---
\begin{titlepage}
    \maketitle
    \vfill
    \begin{center}
        % \includegraphics[width=0.6\textwidth]{logo_ecole.png} % Add your school logo here
        \vspace{1cm}
        \Large \textbf{Master 1 - Financial Engineering \& Machine Learning}
    \end{center}
\end{titlepage}

\newpage
\tableofcontents
\newpage

% --- MAIN CONTENT ---
\onehalfspacing % Set line spacing to 1.5

% ==================================================================
\section{Introduction}
% ==================================================================

\subsection{Project Objective and Business Case}
As M1 students specializing in Machine Learning and Financial Engineering, we are acutely aware of the impact that information—and sentiment—has on market dynamics. The primary objective of this project is to design, implement, and rigorously compare a suite of machine learning models capable of classifying the sentiment of financial texts.

The "Twitter Financial News Sentiment Dataset" we selected, sourced from Kaggle, provides a collection of financial news headlines and tweets, each labeled as \textbf{positive (1)}, \textbf{negative (0)}, or \textbf{neutral (2)}.

\paragraph{Business Case:} The "business case" for this project is compelling and directly relevant to our field of study. Financial markets are driven by information. The ability to automatically and accurately parse thousands of news articles, earnings reports, and social media posts to extract a quantifiable sentiment score is a significant asset. This can be used to:
\begin{itemize}
    \item \textbf{Support Algorithmic Trading:} Feed sentiment data as a feature into quantitative trading models.
    \item \textbf{Enhance Risk Management:} Detect systemic negative sentiment surrounding an asset or market.
    \item \textbf{Inform Investment Decisions:} Provide portfolio managers with a real-time pulse of market perception.
\end{itemize}
Our goal is to build a robust classifier that forms the core of such a system.

\subsection{Methodology Overview}
To ensure a structured and comprehensive analysis, we followed the 3-step methodology outlined in our Project Guidelines (ML-2025):
\begin{enumerate}
    \item \textbf{Step 1: Data Exploration and Preprocessing.} We began by performing a deep dive into the dataset to understand its characteristics, identify potential biases (like class imbalance), and build a robust Natural Language Processing (NLP) pipeline to clean and prepare the text.
    \item \textbf{Step 2: Standard Solutions and Baseline.} We implemented several "classic" machine learning algorithms (Logistic Regression, Random Forest, Naive Bayes) to establish a strong, reliable performance baseline.
    \item \textbf{Step 3: Advanced Solutions and Improvement.} We first optimized our best-performing classical model. Then, as required, we implemented a more complex Deep Learning algorithm (an LSTM network) to determine if a state-of-the-art solution could provide a tangible performance boost.
\end{enumerate}

\newpage
% ==================================================================
\section{Step 1: Exploratory Data Analysis \& Preprocessing}
% ==================================================================

\subsection{Initial Data Inspection}
We began by loading the `sent_train.csv` dataset into a pandas DataFrame. Our initial analysis confirmed that the dataset was clean and well-structured. We had two primary columns: 'text' (our feature, $X$) and 'label' (our target, $y$).

Crucially, an inspection for missing data revealed \textbf{zero null values}. This was excellent news, as it meant we could proceed directly to analysis without needing to devise imputation strategies.

\subsection{Target Variable Analysis: Class Imbalance}
Our next step was to analyze the distribution of our target variable, 'label'. This is a critical step in any classification project, as a significant imbalance can severely bias a model.

The distribution of the three classes was as follows:
\begin{itemize}
    \item \textbf{Label 2 (Neutral):} $\approx 60.5\%$ of the dataset
    \item \textbf{Label 1 (Positive):} $\approx 25.1\%$ of the dataset
    \item \textbf{Label 0 (Negative):} $\approx 14.4\%$ of the dataset
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{repartition of classes.png}
    \caption{Distribution of Sentiment Classes (0=Negative, 1=Positive, 2=Neutral).}
    \label{fig:distribution}
\end{figure}

This visualization (Figure \ref{fig:distribution}) confirmed our primary challenge: the dataset is \textbf{highly imbalanced}. The 'Neutral' class is dominant, while the 'Negative' class is significantly under-represented.

\paragraph{Implication:} If left unaddressed, our models would likely develop a strong bias towards predicting 'Neutral' to maximize their overall accuracy. This would make them perform poorly on the minority classes (Positive and Negative), which, in a financial context, are often the most actionable and important signals. This finding made it clear that a re-sampling strategy would be essential during the modeling phase.

\subsection{Natural Language Processing (NLP) Pipeline}
Text data is inherently unstructured. To prepare it for our models, we designed a comprehensive "sanitization" pipeline, which we applied to every text entry in our dataset. This function performed the following critical steps:
\begin{enumerate}
    \item \textbf{Lowercasing:} Converted all text to lowercase to ensure that "Stock" and "stock" are treated as the same word.
    \item \textbf{URL and Special Character Removal:} We used regular expressions to strip all URLs, Twitter handles, and any non-alphabetic characters (e.g., punctuation, numbers, symbols). This isolates the words that carry sentiment.
    \item \textbf{Stopword Removal:} We removed common English stopwords (e.g., 'the', 'is', 'a', 'in') using the NLTK library. These words add little to no semantic value for sentiment analysis and create unnecessary noise.
    \item \textbf{Lemmatization:} We used the WordNetLemmatizer from NLTK. This process reduces words to their root or dictionary form (e.g., "running" becomes "run", "better" becomes "good"). This is more sophisticated than simple stemming and helps consolidate our vocabulary, reducing the overall dimensionality of our data.
\end{enumerate}
The output of this pipeline was a new column, 'cleaned\_text', which served as the foundation for all subsequent feature engineering.

\newpage
% ==================================================================
\section{Feature Engineering: Representing Text}
% ==================================================================

This section details one of the most critical aspects of our project: converting the 'cleaned\_text' data into a numerical format that machine learning algorithms can understand. We explored two distinct, high-level strategies as required by our project guidelines.

\subsection{Method 1: TF-IDF with N-grams (For Classical Models)}
For our baseline models, we chose the \textbf{Term Frequency-Inverse Document Frequency (TF-IDF)} vectorization strategy. This is a powerful and standard technique that improves upon the simpler "Bag of Words" (BoW) model.

\begin{itemize}
    \item \textbf{Term Frequency (TF):} This is the simple count of how many times a word appears in a single document (a single tweet or headline).
    \item \textbf{Inverse Document Frequency (IDF):} This is the key insight. It measures how rare a word is across the *entire* corpus. Words that appear everywhere (like "market" or "stock" in our financial dataset) are given a very low weight, while words that are rare but appear frequently in a specific document are given a high weight.
\end{itemize}

TF-IDF thus identifies words that are \textbf{discriminative} and characteristic of a particular document.

\paragraph{Enhancement with N-grams:} We further improved this by setting our vectorizer to use `ngram_range=(1, 2)`. This means it didn't just capture individual words (unigrams), but also pairs of adjacent words (bigrams). This is vital for sentiment analysis. For example:
\begin{itemize}
    \item The unigrams "not" and "good" are individually neutral or positive.
    \item The bigram "not good" is clearly negative.
\end{itemize}
By capturing this local context, our model gains a much more nuanced understanding. We limited the vocabulary to the top 5,000 most frequent N-grams to maintain computational feasibility.

\subsection{Method 2: Word Embeddings (For Deep Learning)}
For our advanced model (the LSTM), a sparse, high-dimensional TF-IDF matrix is not a suitable input. Deep Learning models thrive on \textbf{dense, semantic representations}. For this, we used \textbf{Word Embeddings}.

A word embedding is a learned, low-dimensional vector (e.g., 100 dimensions) where each dimension represents some latent "feature" of the word. The key concept is that words used in similar contexts will have similar vectors. For example, the vectors for "buy", "purchase", and "acquire" would be close to each other in this vector space.

To create embeddings tailored to our specific domain, we trained our own \textbf{Word2Vec} model (using the `gensim` library) on our entire 'cleaned\_text' corpus. This provided us with a 100-dimensional vector for every word in our vocabulary, capturing the specific nuances of financial language present in our data. This formed the basis of our LSTM's "understanding" of the text.

\subsection{Addressing Imbalance: The SMOTE Strategy}
With our representation methods defined, we returned to the critical issue of class imbalance. We chose to use the \textbf{Synthetic Minority Over-sampling Technique (SMOTE)}.

Instead of just duplicating rare samples (which can lead to overfitting), SMOTE intelligently creates new, \textit{synthetic} samples. It does this by finding an data point from a minority class, identifying its nearest neighbors, and then creating a new point along the line segment connecting them.

We applied SMOTE to our TF-IDF data, resulting in a new, perfectly balanced training set where all three classes (Negative, Positive, Neutral) had an equal number of samples. This ensures our model would pay equal attention to all classes during training.

\newpage
% ==================================================================
\section{Step 2: Standard Solutions \& Baseline Performance}
% ==================================================================

With our data preprocessed, vectorized (using TF-IDF), and balanced (using SMOTE), we split it into an 80\% training set and a 20\% testing set. We then trained our three baseline models.

\subsection{Model 1: Logistic Regression}
Logistic Regression is a powerful, linear model that is often a surprisingly strong baseline for text classification. It is efficient to train and highly interpretable.

\subsection{Model 2: Random Forest}
As a representative for non-linear, ensemble methods, we implemented a Random Forest Classifier. By building a multitude of decision trees, it can capture more complex interactions between features that a linear model might miss.

\subsection{Model 3: Multinomial Naive Bayes}
This is a classic algorithm for text classification. It's based on Bayes' theorem and makes a "naive" assumption of conditional independence between features (words). It is known for its speed and good performance on text data.

\subsection{Baseline Results and Model Selection}
We evaluated all three models on our unseen test set. The performance was compared using standard classification metrics, with a focus on \textbf{Accuracy} and the \textbf{F1-Score}, which provides a balanced measure of precision and recall.

\begin{table}[H]
    \centering
    \small % ou \footnotesize, \scriptsize, etc.
    \caption{Baseline Model Performance Comparison (on Test Set)}
    \label{tab:baseline}
    \begin{tabular}{@{}lcccc@{}}
        \toprule
        \textbf{Model} & \textbf{Accuracy} & \textbf{F1-Score (0-Neg)} & \textbf{F1-Score (1-Pos)} & \textbf{F1-Score (2-Neu)} \\ \midrule
        \textbf{Logistic Regression} & \textbf{84\%} & \textbf{0.85} & \textbf{0.79} & \textbf{0.87} \\
        Random Forest & 83\% & 0.85 & 0.80 & 0.84 \\
        Multinomial Naive Bayes & 79\% & 0.79 & 0.72 & 0.85 \\ \bottomrule
    \end{tabular}
\end{table}


\paragraph{Analysis:} The results in Table \ref{tab:baseline} were clear.
\begin{itemize}
    \item The \textbf{Logistic Regression} model emerged as our strongest baseline, achieving \textbf{84\% overall accuracy} and demonstrating robust, balanced F1-scores across all three classes.
    \item The Random Forest was a close second at 83\% accuracy, but was significantly more computationally expensive to train.
    \item The Naive Bayes model, while fast, clearly lagged behind in overall performance, particularly in identifying the 'Positive' class.
\end{itemize}

Based on this evidence, we selected the \textbf{Logistic Regression} model as our "champion" model to carry forward into the optimization and comparison phase.

\newpage
% ==================================================================
\section{Step 3: Model Improvement \& Advanced Solutions}
% ==================================================================

This final step of our project involved two distinct efforts: first, to perfect our best classical model, and second, to challenge it with a complex, state-of-the-art Deep Learning architecture.

\subsection{Optimization: Hyperparameter Tuning}
We first sought to optimize our champion Logistic Regression model using `GridSearchCV`. This method exhaustively tests a "grid" of specified hyperparameters to find the optimal combination. We focused on tuning:
\begin{itemize}
    \item \textbf{`C` (Regularization Strength):} This parameter controls the trade-off between model complexity and error. We tested values from 0.1 to 100.
    \item \textbf{`solver`:} We tested different optimization algorithms, such as 'liblinear' and 'saga'.
\end{itemize}

\paragraph{Results:} After a comprehensive 5-fold cross-validation search, the grid search identified the best parameters. When we re-evaluated this "optimized" model on our test set, it achieved an \textbf{accuracy of 84\%}.

This was a fascinating and positive result. The fact that the performance was identical to our baseline model did not signify failure; on the contrary, it demonstrated that our initial, simple model was already \textbf{highly robust and near-optimal}. This gave us immense confidence in this model as a stable and reliable solution.

\subsection{Advanced Model: Long Short-Term Memory (LSTM) Network}
As per the project guidelines, we next implemented a Deep Learning solution. We chose an \textbf{LSTM (Long Short-Term Memory)} network, which is a type of Recurrent Neural Network (RNN) specifically designed to handle sequential data and capture long-range dependencies—ideal for understanding the contextual flow of a sentence.

\subsubsection{Architecture and Data Pipeline}
Our pipeline for this model was entirely different and more complex:
\begin{enumerate}
    \item \textbf{Tokenization:} We converted our cleaned text into sequences of integers, where each integer maps to a unique word in our vocabulary.
    \item \textbf{Padding:} We "padded" all sequences to a uniform length of 100 tokens, ensuring they could be processed in uniform batches.
    \item \textbf{Embedding Matrix:} We used our custom-trained Word2Vec model to create an embedding matrix. This matrix "injects" our learned semantic knowledge into the model.
    \item \textbf{SMOTE (Train-Only):} Critically, we applied SMOTE \textit{only} to the training split, *after* the train-test split, to prevent data leakage.
    \item \textbf{Model Architecture:} Our Keras model was built with:
        \begin{itemize}
            \item An \textbf{Embedding Layer}, loaded with our Word2Vec weights and set to `trainable=False`.
            \item An \textbf{LSTM Layer} with 64 units and recurrent dropout for regularization.
            \item A final \textbf{Dense Output Layer} with 3 neurons (one for each class) and a `softmax` activation.
        \end{itemize}
\end{enumerate}

\subsubsection{LSTM Performance and Critical Analysis}
We trained the LSTM model for 10 epochs, monitoring its performance on our validation (test) set.

\paragraph{Results:} The performance of the LSTM was \textbf{extremely poor}. The validation accuracy was significantly lower than our 84\% baseline, and the validation loss indicated that the model was not effectively learning the task.

This was perhaps the \textbf{most important finding of our entire project}. It provided a crucial, practical lesson in machine learning that academic theory alone cannot.

\newpage
% ==================================================================
\section{Discussion: The "Simple" Model's Triumph}
% ==================================================================

The failure of our advanced Deep Learning model was not a failure of the project; it was the project's most salient conclusion. It forced us to ask: \textbf{Why did our "simple" Logistic Regression model so decisively outperform a complex LSTM?}

We, as a team, concluded the following:
\begin{enumerate}
    \item \textbf{The Power of Feature Engineering:} Our first approach (TF-IDF + N-grams) was a masterpiece of feature engineering. It was perfectly suited to the task. It explicitly told the model which keywords and key phrases (like "downgrade" or "not good") were important. The Logistic Regression model could then easily learn to assign high negative weights to these features.
    \item \textbf{The Nature of the Data:} Financial headlines and tweets are short, dense, and keyword-driven. The sentiment is often explicit, not hidden in deep, subtle, or long-range context. The LSTM's main strength—understanding long, flowing narratives—was simply not required for this task.
    \item \textbf{Data vs. Complexity:} The Word2Vec/LSTM approach attempts to learn "meaning" and "semantics" from the ground up. This requires a massive amount of data and careful tuning. Our TF-IDF approach, by contrast, relies on a simpler, count-based heuristic that was, for this problem, far more direct and effective.
\end{enumerate}

This project was a critical, hands-on lesson demonstrating that model complexity is not a goal in itself, nor is it a guarantee of superior performance. A deep understanding of the problem domain, combined with robust preprocessing and intelligent feature engineering, proved to be far more impactful.

% ==================================================================
\section{Final Conclusion \& Recommendation}
% ==================================================================

We, Pierre-Antoine Serrier, Yasmine Peraldi, and Rhea Saliba, began this project with the goal of building an effective sentiment classifier for financial news. We successfully navigated the entire machine learning pipeline, from data exploration and cleaning to advanced model implementation.

Our most critical finding was the severe class imbalance in our dataset, which we corrected using the SMOTE technique.

After comparing three baseline models, we identified \textbf{Logistic Regression} as the top performer. Our subsequent optimization of this model confirmed its stability, resulting in a final, robust classifier with \textbf{84\% accuracy} on unseen data.

Our exploration into Deep Learning with an LSTM network served as a powerful counter-example, demonstrating that for this specific problem, a state-of-the-art complex model was inferior to a well-engineered classical one.

\paragraph{Final Recommendation:}
For the defined business case of classifying financial sentiment from headlines and tweets, we unequivocally recommend the \textbf{Optimized Logistic Regression model trained on SMOTE-balanced, TF-IDF (1,2-gram) features}. It is fast, highly accurate, stable, and more interpretable, representing the most effective and efficient solution identified in our comprehensive study.

\end{document}